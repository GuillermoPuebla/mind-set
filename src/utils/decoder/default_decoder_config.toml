# options: 'classification', 'regression'
task_type = ""
# Specify what GPU index to use. Write "cpu" to run on CPU. 
gpu_num = 0

[network]
# Either "resnet152_decoder" or "resnet152_decoder_residual".
# resnet152_decoder will append several 1 layer module across different computation stages of the ResNet152.
# resnet152_decoder_residual will append more complex modules consisting of residual blocs, each containing two convolutional layers.
architecture_name = "resnet152_decoder"
# # Only used for regression method, and should be equal to the number of values the network should predict (generally this is 1). For classification, the number of outputs is automatically set equal to the number of classes based on the [training.dataset] configs (below).
# decoder_outputs = 1
# Whether to use the ResNet152 version pretrained with ImageNet.
imagenet_pretrained = true
# If you want to load a state_dict of a pretrained ResNet152 with decoders, specify the path here. If you use this, "imagenet_pretraing" will be ignored. 
state_dict_path = false
batch_size = 64


[training]
# If train_id == false, then run_id will be set equal to current datetime.
# Other key value pairs will be added in `run_info` by the train script.
train_id = false
learning_rate = 1e-5
weight_decay = 0
# If you want to continue training from a previous session, set this flag to the path of the optimizer (e.g. "models/decoder/id/checkpoint_optimizer.pt"). You also might want to change the network.load_state_dict above. Note that if you haven't saved the optimizer state, you can still continue training with a loaded model - it jut won't be as good at the beginning when you stopped training. 
optimizers_path = false
save_trained_model = true
# If the following is false, a model and optimizer will be only saved at the end of the training session, not at every epoch. Ignored if save_trained_model is false. 
save_at_epoch_end = true
# If True, it will evaluate the datasets specified as [[datasets.validation]] at some intervals during training. This will increase training time but could be useful for checking whether training is converging. 
evaluate_during_training = false

# Example of validation datasets. Note that these are based on an annotation.csv file, created together with every dataset in MindSet. In the annotation file there is always a "Path" factor which contains the path of each image, and many factors. You need to specify which factor to use for classification in `label_cols`. 
# Note: for a classification task, we expect ALL datasets (training and validation) to have the same number of classes.

# If true, it will normalize the labels to be between the range. Useful for regression task in which the values to be predicted have very large/small values.
# Note that this normalization will be applied to ALL datasets (both training and validation).
normalize_labels = false
# if provided, it will assume that the labels is within this range and will normalize to the provided range. E-g- [-10, 255]
# if false, it will not auto detect the max and min values of the labels
label_range = false
# The labels will be normalized te be within these values.
normalize_range = [-1, 1]


[training.dataset]
name = "dataset1"
annotation_file = "path/to/annotation/file.csv"
img_path_col_name = "Path"
# For a REGRESSION task: Use either a string indicating the factor to predict, or a list of string if you want to predict multiple factors. The number of output in the decoder will be automatically set to the number of factors to predict. 
# For a classification task, a string indicating the factor to use as a class label. The number of output in the decoder will be automatically set equal to the number of classes. 
label_cols = [""]
filters = {}      # { Type = "random" }

# Example of validation datasets.
# [[eval.datasets]]
# name = "name_val1"
# annotation_file = ""
# img_path_col_name = "Path"
# label_cols = ["val1"]
# filters = {} # or filters = { Type = "blabla"} 

# You can use more than one validation dataset
# [[eval.datasets]]
# name = "name_val2"
# annotation_file = ""
# img_path_col_name = "Path"
# label_cols = ["val2"]
# filters = {} # or filters = { Type = "blabla"} 


[training.stopping_conditions]
# Stop when this epoch/loss/accuracy. For loss and accuracy it will not simply stop when the specified value is reached, but when the Exponential Moving Average of the loss/accuracy is at least the target value or less/more (for loss/accuracy) for at least 200 batches. This is done to avoid stopping when some lucky batches is encountered.
stop_at_epoch = 50
stop_at_loss = false
stop_at_accuracy = false

[training.monitoring]
# If you want to log in neptune.ai, specify the project name. You need an account there + you need to have created a project with the correct name through their UI + You need to set up your API token: \nhttps://docs.neptune.ai/getting-started/installation. The API should be in an environmental variable called NEPTUNE_API_TOKEN.
# If everything is setup correctly, put your project name here, and you should get a lot of good logging for your training session'
neptune_proj_name = false


[saving_folders]
# The paths for saving the results and the model. If false, no saving will occur. Notice that, during training, the `train_id` will be appended to these folders. 
results_folder = 'results/decoder/'
model_output_folder = 'models/decoder'

[transformation]
# Fill color is the value used when performing transformations such as rotation. Will be ignored if no transformation is applied. 
fill_color = [0, 0, 0]

[transformation.values]
# The range to which apply each transformation. An uniform random variable will be drawn for each parameter. Translation is expressed in fraction of the image size for both horizontal and vertical translation. Scale represents the scaling factor (1 will leave the image unchanged). Rotation is in degree
# Set each one of these to "false" to not apply the respective transformation. 
translation_X = [-0.2, 0.2]
translation_Y = [-0.2, 0.2]
scale = [0.7, 1.3]
rotation = [0, 360]
